{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkhanna\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "from langua import Predict\n",
    "from pandas import read_csv\n",
    "from pandas import Series\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from keras import constraints\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # Read the data\n",
    "    train_data_raw = read_csv(\"./data/train.csv\");\n",
    "    test_data_raw = read_csv(\"./data/test.csv\")\n",
    "    labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    y_train = train_data_raw[labels].values\n",
    "    x_train = train_data_raw[\"comment_text\"]\n",
    "    x_test = test_data_raw[\"comment_text\"]\n",
    "    output = read_csv('data/sample_submission.csv')\n",
    "    return x_train, y_train, x_test\n",
    "\n",
    "x_train, y_train, x_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data points which are not english\n",
    "def translate_input():\n",
    "    input_file_path = './data/train.csv'\n",
    "    output_file_path = './data/train_translated.csv'\n",
    "    data = read_csv(input_file_path)\n",
    "    data['lang'] = Series('en', index=data.index)\n",
    "    p = Predict()\n",
    "    for index, point in data.iterrows():\n",
    "        comment = point['comment_text']\n",
    "        try:\n",
    "            language_detected = p.get_lang(comment)\n",
    "            if language_detected != 'en':\n",
    "                data.set_value(index, 'lang', 'non_en')\n",
    "        except:\n",
    "            data.set_value(index, 'lang', 'non_en')\n",
    "    data = data[data.lang != 'non_en']\n",
    "    data.to_csv(output_file_path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vkhanna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  del sys.path[0]\n",
      "C:\\Users\\vkhanna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "translate_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dimensions = 100\n",
    "\n",
    "# get the embedding from glove file \n",
    "pretrained_vectors = {}\n",
    "f = open('./data/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ', dimensions)\n",
    "    word = values[0]\n",
    "    probabilities = np.asarray(values[1:], dtype='float32')\n",
    "    pretrained_vectors[word] = probabilities\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(pretrained_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2519371 word vectors.\n"
     ]
    }
   ],
   "source": [
    "dimensions = 300\n",
    "\n",
    "# Get embeddings\n",
    "pretrained_vectors = {}\n",
    "f = open('data/wiki.en.vec', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.rstrip().rsplit(' ', dimensions)\n",
    "    word = values[0]\n",
    "    probabilities = np.asarray(values[1:], dtype='float32')\n",
    "    pretrained_vectors[word] = probabilities\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(pretrained_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following are sample words in the glove embeddings\n",
    "print(pretrained_vectors['kill'])\n",
    "print(pretrained_vectors['murder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "train_data_raw =  read_csv(\"./data/train_translated.csv\");\n",
    "test_data_raw =  read_csv(\"./data/test.csv\")\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train_data_raw[labels].values\n",
    "x_train = train_data_raw[\"comment_text\"]\n",
    "x_test  = test_data_raw[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4341"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) - len(train_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = Tokenizer(char_level=False, oov_token=None, num_words=None, split=' ', lower=True, \n",
    "                      filters='!\"#$%&()*+,\\t-./:;\\n<=>?@[\\]^_`{|}~ ', document_count=0)\n",
    "\n",
    "list_x_train = list(x_train)\n",
    "tknzr.fit_on_texts(list_x_train)\n",
    "\n",
    "train_seq = tknzr.texts_to_sequences(x_train)\n",
    "test_seq = tknzr.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "# max comment length\n",
    "print(max(len(sequence) for sequence in train_seq))\n",
    "# fixing the max comment size to cover 90% of the comments \n",
    "processing_length = int(np.percentile([len(sequence) for sequence in train_seq], 90))\n",
    "print(processing_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(train_seq, maxlen=processing_length, padding='post', value=0.0, truncating='post')\n",
    "padded_X_test = pad_sequences(test_seq, maxlen=processing_length, padding='post', value=0.0, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  A Bisexual, like a homosexual or a heterosexual, is not defined by sexual activity. (Much like a 15 year old boy who is attracted to a girl sexually but has never had sex is still straight). A person who is actually sexually attracted/aroused by the same sex as well as the opposite sex is bisexual.\n",
      "Tokenized Comment: [6, 10921, 51, 6, 3298, 26, 6, 9808, 9, 15, 1763, 33, 1951, 2185, 132, 51, 6, 666, 405, 349, 1671, 63, 9, 8172, 3, 6, 1995, 7786, 27, 44, 194, 100, 1402, 9, 156, 1683, 6, 221, 63, 9, 208, 7786, 8172, 22207, 33, 1, 139, 1402, 18, 101, 18, 1, 2087, 1402, 9, 10921]\n"
     ]
    }
   ],
   "source": [
    "for index, (token, token_index) in enumerate(zip(x_train[60:61], train_seq[60:61])):\n",
    "    print('Comment:  {}'.format(token))\n",
    "    print('Tokenized Comment: {}'.format(token_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    embed_matrix = np.zeros((len(tknzr.word_index) + 1, dimensions))\n",
    "    for word, index in tknzr.word_index.items():\n",
    "        vector = pretrained_vectors.get(word)\n",
    "        if vector is not None:\n",
    "            embed_matrix[index] = vector\n",
    "    return embed_matrix\n",
    "\n",
    "embd_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17406   -0.094699  -0.44253    0.10427   -0.28       0.37218\n",
      " -0.02919    0.047345   0.0097075  0.2402    -0.20904    0.21263\n",
      " -0.16953   -0.22599    0.030993  -0.14778    0.012086  -0.24262\n",
      " -0.10528   -0.14844   -0.081931   0.11868   -0.5627    -0.34811\n",
      " -0.087365  -0.08099   -0.15424   -0.015826  -0.22876    0.33089\n",
      " -0.037312   0.28776   -0.2827    -0.01459    0.13523   -0.14421\n",
      "  0.23173   -0.43755    0.061313  -0.26361    0.32181    0.1504\n",
      "  0.1521     0.17965    0.13317    0.056412  -0.0087446  0.13166\n",
      " -0.008926   0.037683   0.0090371 -0.0081518  0.13471   -0.071495\n",
      " -0.32246    0.22833    0.1576     0.087777  -0.27286    0.15595\n",
      " -0.52822    0.21661    0.050134   0.46708   -0.10385    0.15519\n",
      " -0.055313   0.42306    0.087317  -0.043844   0.29568   -0.056677\n",
      " -0.13641   -0.49272    0.072833   0.017133   0.041882  -0.11428\n",
      " -0.13504   -0.013326   0.5042    -0.17995   -0.034001   0.15967\n",
      " -0.1254     0.35338   -0.053087  -0.025029   0.10062   -0.22975\n",
      "  0.096282  -0.038608  -0.16653   -0.17763    0.030433   0.16691\n",
      "  0.13055   -0.095244  -0.066601  -0.15682    0.15476   -0.21998\n",
      " -0.12169   -0.14168    0.055819  -0.19313   -0.12906   -0.26187\n",
      "  0.33145    0.10918   -0.12587   -0.21678   -0.20769   -0.095288\n",
      " -0.1059    -0.30257    0.44297   -0.037154   0.21445   -0.11362\n",
      "  0.21973    0.26357   -0.059591   0.12013   -0.36635   -0.27048\n",
      " -0.59786    0.15001    0.013269   0.05716   -0.37491    0.21666\n",
      " -0.14999    0.39083    0.057356   0.42294    0.12382    0.23847\n",
      " -0.06073   -0.26319   -0.023226   0.40473    0.10055   -0.25598\n",
      "  0.045131   0.34859    0.1275    -0.48343    0.077924   0.33297\n",
      "  0.20915   -0.15842   -0.36561    0.07589   -0.030869  -0.091788\n",
      " -0.27258    0.27651    0.19533   -0.15839   -0.068947  -0.086613\n",
      " -0.20616    0.19863   -0.41926   -0.041699  -0.1152     0.10429\n",
      "  0.0986     0.50431   -0.10307   -0.19215   -0.45906   -0.058296\n",
      " -0.012447   0.20842   -0.091112   0.08212    0.050698   0.018116\n",
      " -0.16298   -0.15603   -0.061481   0.17918   -0.015555   0.22519\n",
      " -0.60858    0.10623    0.28127   -0.25997   -0.1269    -0.64115\n",
      "  0.26393    0.1905    -0.0218     0.18569   -0.25188   -0.07963\n",
      " -0.55653    0.19646   -0.055976  -0.14718    0.23322    0.073171\n",
      "  0.15463   -0.21303   -0.19812   -0.066552   0.32815   -0.13344\n",
      "  0.74888    0.10984    0.074924  -0.12408    0.20139   -0.25632\n",
      " -0.34972    0.074837  -0.018315  -0.05396   -0.51713   -0.22018\n",
      "  0.17969    0.18253   -0.031741  -0.12223   -0.37133    0.064362\n",
      " -0.065088  -0.29345    0.046537  -0.16858    0.26601    0.58167\n",
      "  0.24462    0.24862   -0.087782   0.38763    0.064217  -0.12299\n",
      "  0.19261    0.28059    0.15148    0.35187    0.35899    0.38955\n",
      " -0.14202    0.12535   -0.24525   -0.16391   -0.04106   -0.13119\n",
      " -0.33627    0.20134    0.33281   -0.1294     0.068988  -0.1343\n",
      " -0.12449   -0.27107    0.26233    0.29697    0.083506  -0.031643\n",
      " -0.30126   -0.23617    0.028125  -0.20742    0.46005   -0.25591\n",
      "  0.043862  -0.10819   -0.28577   -0.01169   -0.084718  -0.30565\n",
      "  0.54617    0.1334    -0.3364     0.1555    -0.033865   0.01653\n",
      "  0.16095   -0.16507    0.115      0.40149    0.058194   0.092781\n",
      " -0.090828   0.088708   0.093722   0.13276    0.17901   -0.35432\n",
      " -0.12229    0.01806    0.072687   0.34047    0.33325   -0.28109  ]\n",
      "[-0.17406    -0.094699   -0.44253001  0.10427    -0.28        0.37218001\n",
      " -0.02919     0.047345    0.0097075   0.2402     -0.20904     0.21263\n",
      " -0.16953    -0.22599     0.030993   -0.14778     0.012086   -0.24262001\n",
      " -0.10528    -0.14844    -0.081931    0.11868    -0.56269997 -0.34810999\n",
      " -0.087365   -0.08099    -0.15424    -0.015826   -0.22876     0.33089\n",
      " -0.037312    0.28775999 -0.2827     -0.01459     0.13523    -0.14421\n",
      "  0.23173    -0.43755001  0.061313   -0.26361001  0.32181001  0.1504\n",
      "  0.1521      0.17964999  0.13316999  0.056412   -0.0087446   0.13166\n",
      " -0.008926    0.037683    0.0090371  -0.0081518   0.13471    -0.071495\n",
      " -0.32246     0.22833     0.1576      0.087777   -0.27285999  0.15594999\n",
      " -0.52822     0.21661     0.050134    0.46708    -0.10385     0.15519001\n",
      " -0.055313    0.42306     0.087317   -0.043844    0.29567999 -0.056677\n",
      " -0.13641    -0.49272001  0.072833    0.017133    0.041882   -0.11428\n",
      " -0.13504    -0.013326    0.50419998 -0.17995    -0.034001    0.15967\n",
      " -0.12540001  0.35337999 -0.053087   -0.025029    0.10062    -0.22975001\n",
      "  0.096282   -0.038608   -0.16653    -0.17763001  0.030433    0.16690999\n",
      "  0.13055    -0.095244   -0.066601   -0.15682     0.15476    -0.21998\n",
      " -0.12169    -0.14168     0.055819   -0.19313    -0.12906    -0.26187\n",
      "  0.33144999  0.10918    -0.12587    -0.21678001 -0.20769    -0.095288\n",
      " -0.1059     -0.30256999  0.44297001 -0.037154    0.21445    -0.11362\n",
      "  0.21973     0.26357001 -0.059591    0.12013    -0.36635    -0.27048001\n",
      " -0.59785998  0.15001     0.013269    0.05716    -0.37491     0.21665999\n",
      " -0.14999001  0.39083001  0.057356    0.42293999  0.12382     0.23847\n",
      " -0.06073    -0.26319    -0.023226    0.40472999  0.10055    -0.25598001\n",
      "  0.045131    0.34858999  0.1275     -0.48343     0.077924    0.33296999\n",
      "  0.20915    -0.15842    -0.36561     0.07589    -0.030869   -0.091788\n",
      " -0.27258     0.27651     0.19532999 -0.15839    -0.068947   -0.086613\n",
      " -0.20615999  0.19863001 -0.41926    -0.041699   -0.1152      0.10429\n",
      "  0.0986      0.50431001 -0.10307    -0.19215    -0.45906001 -0.058296\n",
      " -0.012447    0.20841999 -0.091112    0.08212     0.050698    0.018116\n",
      " -0.16298001 -0.15603    -0.061481    0.17918    -0.015555    0.22519\n",
      " -0.60857999  0.10623     0.28127    -0.25997001 -0.1269     -0.64115\n",
      "  0.26392999  0.19050001 -0.0218      0.18569    -0.25187999 -0.07963\n",
      " -0.55653     0.19645999 -0.055976   -0.14718001  0.23322     0.073171\n",
      "  0.15463001 -0.21303    -0.19812    -0.066552    0.32815    -0.13344\n",
      "  0.74888003  0.10984     0.074924   -0.12408     0.20139    -0.25632\n",
      " -0.34972     0.074837   -0.018315   -0.05396    -0.51713002 -0.22018\n",
      "  0.17969     0.18253    -0.031741   -0.12223    -0.37132999  0.064362\n",
      " -0.065088   -0.29345     0.046537   -0.16858     0.26600999  0.58166999\n",
      "  0.24462     0.24862    -0.087782    0.38762999  0.064217   -0.12299\n",
      "  0.19261     0.28059     0.15148     0.35187     0.35899001  0.38955\n",
      " -0.14202     0.12535    -0.24525    -0.16391    -0.04106    -0.13119\n",
      " -0.33627     0.20134     0.33281001 -0.1294      0.068988   -0.13429999\n",
      " -0.12449    -0.27107     0.26233     0.29697001  0.083506   -0.031643\n",
      " -0.30125999 -0.23616999  0.028125   -0.20742001  0.46004999 -0.25591001\n",
      "  0.043862   -0.10819    -0.28577    -0.01169    -0.084718   -0.30565\n",
      "  0.54617     0.13339999 -0.3364      0.15549999 -0.033865    0.01653\n",
      "  0.16095001 -0.16507     0.115       0.40149     0.058194    0.092781\n",
      " -0.090828    0.088708    0.093722    0.13276     0.17901    -0.35431999\n",
      " -0.12229     0.01806     0.072687    0.34046999  0.33324999 -0.28108999]\n"
     ]
    }
   ],
   "source": [
    "index = tknzr.word_index['kill']\n",
    "print(pretrained_vectors['kill']) \n",
    "print(embd_matrix[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 160, 300)          65599500  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 160, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 156, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                7740      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 366       \n",
      "=================================================================\n",
      "Total params: 65,800,246\n",
      "Trainable params: 200,490\n",
      "Non-trainable params: 65,599,756\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GlobalMaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tknzr.word_index) + 1, dimensions, weights=[embd_matrix], input_length=processing_length, trainable=False))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(layers.Dense(6, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Nadam(lr=0.001), metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_final_train, X_final_test, Y_final_train, Y_final_test] = train_test_split(padded_X_train, y_train, test_size=0.10, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 139707 samples, validate on 15523 samples\n",
      "Epoch 1/6\n",
      "139707/139707 [==============================] - 505s 4ms/step - loss: 0.0677 - binary_accuracy: 0.9760 - val_loss: 0.0466 - val_binary_accuracy: 0.9827\n",
      "Epoch 2/6\n",
      "139707/139707 [==============================] - 506s 4ms/step - loss: 0.0454 - binary_accuracy: 0.9827 - val_loss: 0.0457 - val_binary_accuracy: 0.9826\n",
      "Epoch 3/6\n",
      "139707/139707 [==============================] - 505s 4ms/step - loss: 0.0415 - binary_accuracy: 0.9840 - val_loss: 0.0452 - val_binary_accuracy: 0.9831\n",
      "Epoch 4/6\n",
      "139707/139707 [==============================] - 505s 4ms/step - loss: 0.0383 - binary_accuracy: 0.9848 - val_loss: 0.0485 - val_binary_accuracy: 0.9829\n",
      "Epoch 5/6\n",
      "139707/139707 [==============================] - 504s 4ms/step - loss: 0.0356 - binary_accuracy: 0.9856 - val_loss: 0.0479 - val_binary_accuracy: 0.9830\n",
      "Epoch 6/6\n",
      "139707/139707 [==============================] - 504s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9863 - val_loss: 0.0525 - val_binary_accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "graph = model.fit(X_final_train, Y_final_train, batch_size=64, epochs=6,\n",
    "                  validation_data=(X_final_test, Y_final_test), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"detoxit_model_sigmoid.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "new_model = load_model(\"detoxit_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = new_model.predict(padded_X_test, batch_size=32, verbose=0, steps= None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_input = read_csv(\"./data/sample_submission.csv\")\n",
    "submission_input[labels] = result\n",
    "submission_input.to_csv(\"submission_file_30_oct_cnn_fasttext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[193, 8, 7646]]\n"
     ]
    }
   ],
   "source": [
    "string = 'fuck you bitches'\n",
    "new_string = [string]\n",
    "new_string = tknzr.texts_to_sequences(new_string)\n",
    "print(new_string)\n",
    "new_string = pad_sequences(new_string, maxlen=processing_length, padding='post', truncating='post')\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tknzr, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.3697548e-01, 4.0647509e-03, 3.8161206e-01, 4.9847891e-05,\n",
       "        1.6307604e-01, 1.4221812e-02]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity levels for 'fuck you bitches':\n",
      "Toxic:         100%\n",
      "Severe Toxic:  69%\n",
      "Obscene:       100%\n",
      "Threat:        0%\n",
      "Insult:        99%\n",
      "Identity Hate: 14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction\n",
    "# Print output\n",
    "print(\"Toxicity levels for '{}':\".format(string))\n",
    "print('Toxic:         {:.0%}'.format(prediction[0][0]))\n",
    "print('Severe Toxic:  {:.0%}'.format(prediction[0][1]))\n",
    "print('Obscene:       {:.0%}'.format(prediction[0][2]))\n",
    "print('Threat:        {:.0%}'.format(prediction[0][3]))\n",
    "print('Insult:        {:.0%}'.format(prediction[0][4]))\n",
    "print('Identity Hate: {:.0%}'.format(prediction[0][5]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 156, 300)          62335500  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 2)                 2424      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 18        \n",
      "=================================================================\n",
      "Total params: 62,337,942\n",
      "Trainable params: 2,442\n",
      "Non-trainable params: 62,335,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(len(tknzr.word_index) + 1, dimensions, weights=[embd_matrix], input_length=processing_length, trainable=False))\n",
    "\n",
    "lstm_model.add(LSTM(2))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(layers.Dense(6, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
    "[X_final_train, X_final_test, Y_final_train, Y_final_test] = train_test_split(padded_X_train, y_train, test_size=0.10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 139738 samples, validate on 15527 samples\n",
      "Epoch 1/1\n",
      "139738/139738 [==============================] - 172s 1ms/step - loss: 0.0457 - acc: 0.9827 - val_loss: 0.0509 - val_acc: 0.9817\n"
     ]
    }
   ],
   "source": [
    "graph = model.fit(X_final_train, Y_final_train, batch_size=64, epochs=1,\n",
    "                  validation_data=(X_final_test, Y_final_test), verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(padded_X_test, batch_size=32, verbose=0, steps= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_input = read_csv(\"data/sample_submission.csv\")\n",
    "submission_input[labels] = result\n",
    "submission_input.to_csv(\"submission_file_21_nov_lstm_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
